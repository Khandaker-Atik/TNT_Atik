{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, f1_score\n",
        "\n",
        "def clean_text(text):\n",
        "    if isinstance(text, str):\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http\\S+', '', text)\n",
        "        # Remove hashtags but keep the text\n",
        "        text = re.sub(r'#', '', text)\n",
        "        # Remove special characters\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "    return \"\"\n",
        "\n",
        "def extract_topic(row, topics_list):\n",
        "    text = str(row['Training Topic'])\n",
        "\n",
        "    # Find which topic appears in the text\n",
        "    for topic in topics_list:\n",
        "        if topic.lower() in text.lower():\n",
        "            return topic\n",
        "\n",
        "    # checking hashtags for topic clues\n",
        "    for topic in topics_list:\n",
        "        # Create hashtag variations\n",
        "        hashtag = f\"#{topic.lower().replace(' ', '')}\"\n",
        "        hashtag2 = f\"#{topic.lower().replace(' & ', '').replace(' ', '')}\"\n",
        "        if hashtag in text.lower() or hashtag2 in text.lower():\n",
        "            return topic\n",
        "\n",
        "    # common keywords\n",
        "    keyword_mapping = {\n",
        "        'corruption': 'Corruption',\n",
        "        'election': 'Election',\n",
        "        'vote': 'Election',\n",
        "        'environment': 'Environment',\n",
        "        'climate': 'Environment',\n",
        "        'police': 'Law and Order',\n",
        "        'crime': 'Law and Order',\n",
        "        'women': 'Women Rights',\n",
        "        'gender': 'Women Rights',\n",
        "        'sport': 'Sports',\n",
        "        'terrorism': 'Terrorism',\n",
        "        'education': 'Education',\n",
        "        'religion': 'Religion',\n",
        "        'trade': 'Trade & Commodity Price',\n",
        "        'politics': 'Politics',\n",
        "        'minister': 'Politics',\n",
        "        'government': 'Politics'\n",
        "    }\n",
        "\n",
        "    for keyword, topic in keyword_mapping.items():\n",
        "        if keyword in text.lower():\n",
        "            return topic\n",
        "\n",
        "    # Default to the most common class if no match\n",
        "    return 'Politics'  # Assuming Politics is the most common\n",
        "\n",
        "def main():\n",
        "    topics = [\n",
        "        'Politics', 'Law and Order', 'Governance & Policy Reform', 'Sports',\n",
        "        'Culture & Lifestyle', 'Corruption', 'International affairs', 'Election',\n",
        "        'Environment', 'Natural Disaster', 'Terrorism', 'Mob Justice',\n",
        "        'Women Rights', 'Islamic Fundamentalism', 'National Defence', 'Diplomacy',\n",
        "        'Education', 'Religion', 'Trade & Commodity Price'\n",
        "    ]\n",
        "\n",
        "    # dataset load\n",
        "    try:\n",
        "        df = pd.read_excel('data_and_topics.xlsx', sheet_name='Training Dataset')\n",
        "    except:\n",
        "        print(\"Could not find the Excel file.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "    print(df.head())\n",
        "\n",
        "    # cleaning function apply\n",
        "    df['cleaned_text'] = df['Context'].apply(clean_text)\n",
        "\n",
        "    # extract the topic labels\n",
        "    df['topic'] = df.apply(lambda row: extract_topic(row, topics), axis=1)\n",
        "\n",
        "    # distribution of topics check\n",
        "    print(\"\\nTopic distribution:\")\n",
        "    topic_distribution = df['topic'].value_counts()\n",
        "    print(topic_distribution)\n",
        "\n",
        "    # Plot topic distribution\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    topic_distribution.plot(kind='bar')\n",
        "    plt.title('Topic Distribution')\n",
        "    plt.xlabel('Topic')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('topic_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "    # split into features and labels\n",
        "    X = df['cleaned_text']\n",
        "    y = df['topic']\n",
        "\n",
        "    #train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # STEP 1: BASE MODEL\n",
        "    print(\"\\n--- Building the Base Model ---\")\n",
        "\n",
        "    # Creating TF-IDF features\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "    # training a basic Naive Bayes model\n",
        "    clf = MultinomialNB()\n",
        "    clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    # predict on the training data\n",
        "    y_train_pred = clf.predict(X_train_tfidf)\n",
        "\n",
        "    # Calculate training metrics\n",
        "    train_precision = precision_score(y_train, y_train_pred, average='weighted')\n",
        "    train_recall = recall_score(y_train, y_train_pred, average='weighted')\n",
        "    train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
        "    train_cm = confusion_matrix(y_train, y_train_pred)\n",
        "\n",
        "    print(\"\\nTraining Set Metrics:\")\n",
        "    print(f\"Precision: {train_precision:.4f}\")\n",
        "    print(f\"Recall: {train_recall:.4f}\")\n",
        "    print(f\"F1 Score: {train_f1:.4f}\")\n",
        "\n",
        "    # predict on the test data\n",
        "    y_test_pred = clf.predict(X_test_tfidf)\n",
        "\n",
        "    # calculate test metrics\n",
        "    test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
        "    test_recall = recall_score(y_test, y_test_pred, average='weighted')\n",
        "    test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
        "    test_cm = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "    print(\"\\nTest Set Metrics:\")\n",
        "    print(f\"Precision: {test_precision:.4f}\")\n",
        "    print(f\"Recall: {test_recall:.4f}\")\n",
        "    print(f\"F1 Score: {test_f1:.4f}\")\n",
        "\n",
        "    # classification report\n",
        "    print(\"\\nClassification Report (Test Set):\")\n",
        "    print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "    # base model metrics for comparison\n",
        "    base_metrics = {\n",
        "        'train': {\n",
        "            'precision': train_precision,\n",
        "            'recall': train_recall,\n",
        "            'f1': train_f1\n",
        "        },\n",
        "        'test': {\n",
        "            'precision': test_precision,\n",
        "            'recall': test_recall,\n",
        "            'f1': test_f1\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # synthetic test set\n",
        "    synthetic_data = {\n",
        "        'Context': [\n",
        "            \"Bangladesh government introduces new anti-corruption measures\",\n",
        "            \"Local sports team wins national championship\",\n",
        "            \"New environmental protection law passed\",\n",
        "            \"Women's rights activists protest for equal pay\",\n",
        "            \"Election commission announces dates for upcoming vote\",\n",
        "            \"Police arrest suspect in robbery case\",\n",
        "            \"Religious leaders call for peace and harmony\",\n",
        "            \"Trade relations with neighboring countries improve\",\n",
        "            \"Defense minister announces new security policy\",\n",
        "            \"Educational reforms to improve literacy rates\"\n",
        "        ],\n",
        "        'True_Topic': [\n",
        "            'Corruption',\n",
        "            'Sports',\n",
        "            'Environment',\n",
        "            'Women Rights',\n",
        "            'Election',\n",
        "            'Law and Order',\n",
        "            'Religion',\n",
        "            'Trade & Commodity Price',\n",
        "            'National Defence',\n",
        "            'Education'\n",
        "        ]\n",
        "    }\n",
        "    synthetic_df = pd.DataFrame(synthetic_data)\n",
        "    synthetic_df['cleaned_text'] = synthetic_df['Context'].apply(clean_text)\n",
        "    X_synthetic = synthetic_df['cleaned_text']\n",
        "    y_synthetic = synthetic_df['True_Topic']\n",
        "\n",
        "    # transform and predict on synthetic data\n",
        "    X_synthetic_tfidf = tfidf_vectorizer.transform(X_synthetic)\n",
        "    y_synthetic_pred = clf.predict(X_synthetic_tfidf)\n",
        "\n",
        "    # Calculate metrics on synthetic test set\n",
        "    synthetic_precision = precision_score(y_synthetic, y_synthetic_pred, average='weighted')\n",
        "    synthetic_recall = recall_score(y_synthetic, y_synthetic_pred, average='weighted')\n",
        "    synthetic_f1 = f1_score(y_synthetic, y_synthetic_pred, average='weighted')\n",
        "\n",
        "    print(\"\\nSynthetic Test Set Metrics:\")\n",
        "    print(f\"Precision: {synthetic_precision:.4f}\")\n",
        "    print(f\"Recall: {synthetic_recall:.4f}\")\n",
        "    print(f\"F1 Score: {synthetic_f1:.4f}\")\n",
        "\n",
        "    # save the base model\n",
        "    with open('base_topic_classifier.pkl', 'wb') as f:\n",
        "        pickle.dump(clf, f)\n",
        "    with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
        "        pickle.dump(tfidf_vectorizer, f)\n",
        "\n",
        "    print(\"Base model saved to 'base_topic_classifier.pkl'\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # STEP 2: IMPROVED MODEL\n",
        "    print(\"\\n--- Building Improved Model ---\")\n",
        "\n",
        "    # improved pipeline with feature engineering and better algorithm\n",
        "    pipeline = Pipeline([\n",
        "        ('vect', CountVectorizer(ngram_range=(1, 3), min_df=3, max_df=0.9)),\n",
        "        ('chi2', SelectKBest(chi2, k=2000)),\n",
        "        ('clf', RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42))\n",
        "    ])\n",
        "\n",
        "    # fit improved model\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on training set\n",
        "    y_train_pred_improved = pipeline.predict(X_train)\n",
        "    improved_train_precision = precision_score(y_train, y_train_pred_improved, average='weighted')\n",
        "    improved_train_recall = recall_score(y_train, y_train_pred_improved, average='weighted')\n",
        "    improved_train_f1 = f1_score(y_train, y_train_pred_improved, average='weighted')\n",
        "\n",
        "    print(\"\\nImproved Model - Training Set Metrics:\")\n",
        "    print(f\"Precision: {improved_train_precision:.4f}\")\n",
        "    print(f\"Recall: {improved_train_recall:.4f}\")\n",
        "    print(f\"F1 Score: {improved_train_f1:.4f}\")\n",
        "\n",
        "    # predict on test set\n",
        "    y_test_pred_improved = pipeline.predict(X_test)\n",
        "    improved_test_precision = precision_score(y_test, y_test_pred_improved, average='weighted')\n",
        "    improved_test_recall = recall_score(y_test, y_test_pred_improved, average='weighted')\n",
        "    improved_test_f1 = f1_score(y_test, y_test_pred_improved, average='weighted')\n",
        "\n",
        "    print(\"\\nImproved Model - Test Set Metrics:\")\n",
        "    print(f\"Precision: {improved_test_precision:.4f}\")\n",
        "    print(f\"Recall: {improved_test_recall:.4f}\")\n",
        "    print(f\"F1 Score: {improved_test_f1:.4f}\")\n",
        "\n",
        "    # now detailed classification report\n",
        "    print(\"\\nImproved Model - Classification Report (Test Set):\")\n",
        "    print(classification_report(y_test, y_test_pred_improved))\n",
        "\n",
        "    # predict on synthetic test set\n",
        "    y_synthetic_pred_improved = pipeline.predict(X_synthetic)\n",
        "    improved_synthetic_precision = precision_score(y_synthetic, y_synthetic_pred_improved, average='weighted')\n",
        "    improved_synthetic_recall = recall_score(y_synthetic, y_synthetic_pred_improved, average='weighted')\n",
        "    improved_synthetic_f1 = f1_score(y_synthetic, y_synthetic_pred_improved, average='weighted')\n",
        "\n",
        "    print(\"\\nImproved Model - Synthetic Test Set Metrics:\")\n",
        "    print(f\"Precision: {improved_synthetic_precision:.4f}\")\n",
        "    print(f\"Recall: {improved_synthetic_recall:.4f}\")\n",
        "    print(f\"F1 Score: {improved_synthetic_f1:.4f}\")\n",
        "\n",
        "    # Store improved model metrics\n",
        "    improved_metrics = {\n",
        "        'train': {\n",
        "            'precision': improved_train_precision,\n",
        "            'recall': improved_train_recall,\n",
        "            'f1': improved_train_f1\n",
        "        },\n",
        "        'test': {\n",
        "            'precision': improved_test_precision,\n",
        "            'recall': improved_test_recall,\n",
        "            'f1': improved_test_f1\n",
        "        },\n",
        "        'synthetic': {\n",
        "            'precision': improved_synthetic_precision,\n",
        "            'recall': improved_synthetic_recall,\n",
        "            'f1': improved_synthetic_f1\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Calculating improvement percentages\n",
        "    f1_improvement = (improved_metrics['test']['f1'] - base_metrics['test']['f1']) / base_metrics['test']['f1'] * 100\n",
        "    precision_improvement = (improved_metrics['test']['precision'] - base_metrics['test']['precision']) / base_metrics['test']['precision'] * 100\n",
        "    recall_improvement = (improved_metrics['test']['recall'] - base_metrics['test']['recall']) / base_metrics['test']['recall'] * 100\n",
        "\n",
        "    print(\"\\n--- Model Improvement Results ---\")\n",
        "    print(f\"F1 Score Improvement: {f1_improvement:.2f}%\")\n",
        "    print(f\"Precision Improvement: {precision_improvement:.2f}%\")\n",
        "    print(f\"Recall Improvement: {recall_improvement:.2f}%\")\n",
        "\n",
        "    # comparison visualization\n",
        "    metrics = ['F1 Score', 'Precision', 'Recall']\n",
        "    base_values = [base_metrics['test']['f1'], base_metrics['test']['precision'], base_metrics['test']['recall']]\n",
        "    improved_values = [improved_metrics['test']['f1'], improved_metrics['test']['precision'], improved_metrics['test']['recall']]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    x = np.arange(len(metrics))\n",
        "    width = 0.35\n",
        "\n",
        "    plt.bar(x - width/2, base_values, width, label='Base Model')\n",
        "    plt.bar(x + width/2, improved_values, width, label='Improved Model')\n",
        "\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Model Performance Comparison')\n",
        "    plt.xticks(x, metrics)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_improvement_comparison.png')\n",
        "    plt.close()\n",
        "\n",
        "    # saving the improved model\n",
        "    with open('improved_topic_classifier.pkl', 'wb') as f:\n",
        "        pickle.dump(pipeline, f)\n",
        "\n",
        "    print(\"\\nImproved model saved to 'improved_topic_classifier.pkl'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8980i5XnzK5",
        "outputId": "fee70136-9253-4adc-cd35-5e497d59d664"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (590, 3)\n",
            "                                             Context Training Topic  \\\n",
            "0  Wiping out graft to fatten coffers   Will the ...     Corruption   \n",
            "1  দূর্নীতির আখড়া গোপালগঞ্জ হাসপাতাল,ধরলো দুদক   ...     Corruption   \n",
            "2  মুজিব কিল্লা নির্মাণে অর্থ তছরুপে জড়িতদের বিরু...     Corruption   \n",
            "3  অভিযোগের বিষয়ে জানতে গেলে বোবার অভিনয়...  বিস্...     Corruption   \n",
            "4  ব্রিজ নির্মাণ হলেও দেয়া হয়নি এপ্রোচ সড়ক, গোপাল...     Corruption   \n",
            "\n",
            "   Predicted Topic  \n",
            "0              NaN  \n",
            "1              NaN  \n",
            "2              NaN  \n",
            "3              NaN  \n",
            "4              NaN  \n",
            "\n",
            "Topic distribution:\n",
            "topic\n",
            "Mob Justice                   211\n",
            "Politics                       91\n",
            "Law and Order                  53\n",
            "International affairs          49\n",
            "Religion                       48\n",
            "Islamic Fundamentalism         24\n",
            "National Defence               23\n",
            "Diplomacy                      19\n",
            "Corruption                     17\n",
            "Sports                         14\n",
            "Terrorism                      10\n",
            "Trade & Commodity Price         9\n",
            "Women Rights                    8\n",
            "Election                        5\n",
            "Governance & Policy Reform      4\n",
            "Culture & Lifestyle             2\n",
            "Education                       1\n",
            "Environment                     1\n",
            "Natural Disaster                1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "--- Building the Base Model ---\n",
            "\n",
            "Training Set Metrics:\n",
            "Precision: 0.6376\n",
            "Recall: 0.5742\n",
            "F1 Score: 0.5011\n",
            "\n",
            "Test Set Metrics:\n",
            "Precision: 0.4535\n",
            "Recall: 0.4068\n",
            "F1 Score: 0.3269\n",
            "\n",
            "Classification Report (Test Set):\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                Corruption       0.00      0.00      0.00         5\n",
            "                 Diplomacy       0.00      0.00      0.00         3\n",
            "                  Election       0.00      0.00      0.00         1\n",
            "Governance & Policy Reform       0.00      0.00      0.00         1\n",
            "     International affairs       0.75      0.27      0.40        11\n",
            "    Islamic Fundamentalism       0.00      0.00      0.00         3\n",
            "             Law and Order       1.00      0.07      0.12        15\n",
            "               Mob Justice       0.44      0.92      0.60        39\n",
            "          National Defence       0.00      0.00      0.00         3\n",
            "                  Politics       0.18      0.23      0.20        22\n",
            "                  Religion       0.75      0.25      0.38        12\n",
            "                    Sports       0.00      0.00      0.00         1\n",
            "              Women Rights       0.00      0.00      0.00         2\n",
            "\n",
            "                  accuracy                           0.41       118\n",
            "                 macro avg       0.24      0.13      0.13       118\n",
            "              weighted avg       0.45      0.41      0.33       118\n",
            "\n",
            "\n",
            "Synthetic Test Set Metrics:\n",
            "Precision: 0.0000\n",
            "Recall: 0.0000\n",
            "F1 Score: 0.0000\n",
            "Base model saved to 'base_topic_classifier.pkl'\n",
            "\n",
            "--- Building Improved Model ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:783: UserWarning: k=2000 is greater than n_features=1909. All the features will be returned.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Improved Model - Training Set Metrics:\n",
            "Precision: 0.9306\n",
            "Recall: 0.9343\n",
            "F1 Score: 0.9300\n",
            "\n",
            "Improved Model - Test Set Metrics:\n",
            "Precision: 0.6485\n",
            "Recall: 0.6271\n",
            "F1 Score: 0.6117\n",
            "\n",
            "Improved Model - Classification Report (Test Set):\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                Corruption       1.00      0.60      0.75         5\n",
            "                 Diplomacy       1.00      0.67      0.80         3\n",
            "                  Election       0.00      0.00      0.00         1\n",
            "Governance & Policy Reform       0.00      0.00      0.00         1\n",
            "     International affairs       0.67      0.55      0.60        11\n",
            "    Islamic Fundamentalism       0.25      0.33      0.29         3\n",
            "             Law and Order       0.88      0.47      0.61        15\n",
            "               Mob Justice       0.61      0.92      0.73        39\n",
            "          National Defence       0.00      0.00      0.00         3\n",
            "                  Politics       0.52      0.50      0.51        22\n",
            "                  Religion       0.75      0.50      0.60        12\n",
            "                    Sports       1.00      1.00      1.00         1\n",
            "              Women Rights       1.00      0.50      0.67         2\n",
            "\n",
            "                  accuracy                           0.63       118\n",
            "                 macro avg       0.59      0.46      0.50       118\n",
            "              weighted avg       0.65      0.63      0.61       118\n",
            "\n",
            "\n",
            "Improved Model - Synthetic Test Set Metrics:\n",
            "Precision: 0.1000\n",
            "Recall: 0.1000\n",
            "F1 Score: 0.1000\n",
            "\n",
            "--- Model Improvement Results ---\n",
            "F1 Score Improvement: 87.12%\n",
            "Precision Improvement: 43.01%\n",
            "Recall Improvement: 54.17%\n",
            "\n",
            "Improved model saved to 'improved_topic_classifier.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CqIBsWTwnz8U"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D45QyiLknz_T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}